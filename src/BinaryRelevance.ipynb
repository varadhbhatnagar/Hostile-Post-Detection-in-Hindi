{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdHVQrqyDB50"
   },
   "source": [
    "# Memory Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "E58HaeA6CsM9",
    "outputId": "9a02a375-c264-4c9d-a7a3-b4ae0f8d414a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b17067281b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"K\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"G\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psutil'"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def get_size(bytes, suffix=\"B\"):\n",
    "    factor = 1024\n",
    "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
    "        if bytes < factor:\n",
    "            return f\"{bytes:.2f}{unit}{suffix}\"\n",
    "        bytes /= factor\n",
    "print(\"=\"*40, \"Memory Information\", \"=\"*40)\n",
    "svmem = psutil.virtual_memory()\n",
    "print(f\"Total: {get_size(svmem.total)}\") ; print(f\"Available: {get_size(svmem.available)}\")\n",
    "print(f\"Used: {get_size(svmem.used)}\") ; print(f\"Percentage: {svmem.percent}%\")\n",
    "# keras-team /\n",
    "# keras-contrib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgVfBlvZDEja"
   },
   "source": [
    "# GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "rxlkxvkrCyin",
    "outputId": "fac94447-c55a-4765-a018-5ae38dcde581"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "iLoeOFPPldnP",
    "outputId": "883f2c7d-214e-4cf7-b23e-374b0524566e"
   },
   "outputs": [],
   "source": [
    "!pip install indic-nlp-library\n",
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rXOobmQ2mIDf",
    "outputId": "6cd7088d-f0a1-45cf-81b4-8bf6271070e4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7maoduMmP5i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from indicnlp import common\n",
    "\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"indic_nlp_resources\"\n",
    "\n",
    "# Add library to Python path\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "# Set environment variable for resources folder\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyDODoaWC6KI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pid1-pEcq32"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv', header=0, index_col=0)\n",
    "val_data = pd.read_csv('val.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "DFAe-Z4geoFw",
    "outputId": "4f41880a-30a9-4014-e1fc-b20f5efbdd33"
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "yDG5NtJqhTWg",
    "outputId": "90569738-e571-4587-ce95-44708d45e861"
   },
   "outputs": [],
   "source": [
    "print(val_data.shape)\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avnStOCX1LYw"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "labels_set = {'defamation',\n",
    " 'fake',\n",
    " 'hate',\n",
    " 'non-hostile',\n",
    " 'offensive'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UzE4LSl1M47"
   },
   "source": [
    "##**Using one hot vectors of Emojis, Mentions and Hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agaMrJw2hnF5"
   },
   "outputs": [],
   "source": [
    "# hashtags_set = defaultdict(int)\n",
    "\n",
    "# ct=1\n",
    "# for index, row in train_data.iterrows():\n",
    "#   ht_list = ast.literal_eval(row['hashtags']) \n",
    "#   for ht in ht_list:\n",
    "#     if hashtags_set.get(ht, 0)==0:\n",
    "#       hashtags_set[ht] = ct\n",
    "#       ct+=1\n",
    "\n",
    "# emojis_set = defaultdict(int)\n",
    "# ct=1\n",
    "# for index, row in train_data.iterrows():\n",
    "#   em_list = ast.literal_eval(row['emojis']) \n",
    "#   for em in em_list:\n",
    "#     if emojis_set.get(em, 0)==0:\n",
    "#       emojis_set[em] = ct\n",
    "#       ct+=1\n",
    "\n",
    "# mentions_set = defaultdict(int)\n",
    "# ct=1\n",
    "# for index, row in train_data.iterrows():\n",
    "#   mn_list = ast.literal_eval(row['mentions']) \n",
    "#   for mn in mn_list:\n",
    "#     if mentions_set.get(mn, 0)==0:\n",
    "#       mentions_set[mn] = ct\n",
    "#       ct+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdBlXNn41CHX"
   },
   "source": [
    "##**Using per class counts for Emojis, Mentions and Hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDAW1YEg1AxI"
   },
   "outputs": [],
   "source": [
    "hashtags_set = defaultdict(int)\n",
    "emojis_set = defaultdict(int)\n",
    "mentions_set = defaultdict(int)\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "  ht_list = ast.literal_eval(row['hashtags'])\n",
    "  em_list = ast.literal_eval(row['emojis'])\n",
    "  mn_list = ast.literal_eval(row['mentions']) \n",
    "\n",
    "  for label in [x.strip() for x in row['Labels Set'].split(',')]:\n",
    "    for ht in ht_list:\n",
    "      hashtags_set[ht, label] +=1\n",
    "    for em in em_list:\n",
    "      emojis_set[em, label]+=1\n",
    "    for mn in mn_list:\n",
    "      mentions_set[mn, label]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHlYmRrnpO0o"
   },
   "outputs": [],
   "source": [
    "# print(len(emojis_set))\n",
    "# print(len(hashtags_set))\n",
    "# print(len(mentions_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YQKuRKkJomZg",
    "outputId": "133f0f08-1982-4bfb-c070-89db6f1f67c3"
   },
   "outputs": [],
   "source": [
    "# Creating Frequency Dictionary\n",
    "\n",
    "vocab = defaultdict(int)\n",
    "freqs = defaultdict(int)\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "  for word in indic_tokenize.trivial_tokenize(row['Filtered_Post'], lang='hi'):\n",
    "    vocab[word]+=1\n",
    "    for label in [x.strip() for x in row['Labels Set'].split(',')]:\n",
    "      freqs[word, label] +=1\n",
    "\n",
    "len(freqs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMqYc8wwzlfk"
   },
   "source": [
    "##**Using five class frequencies and other features as one hot vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO1eh_ooJEow"
   },
   "outputs": [],
   "source": [
    "# def generate_train_matrix(train_data):\n",
    "#   train_X = np.empty((0, 3350))\n",
    "\n",
    "#   for index, row in train_data.iterrows():\n",
    "#     x = np.zeros((1, 6))\n",
    "#     x[0, 0] = 1  # Bias Unit\n",
    "#     word_l = indic_tokenize.trivial_tokenize(row['Filtered_Post'], lang='hi')\n",
    "    \n",
    "#     for word in word_l:\n",
    "#         x[0,1] += freqs.get((word,'defamation'),0)\n",
    "#         x[0,2] += freqs.get((word, 'fake'),0)\n",
    "#         x[0,3] += freqs.get((word,'hate'),0)\n",
    "#         x[0,4] += freqs.get((word,'non-hostile'),0)\n",
    "#         x[0,5] += freqs.get((word,'offensive'),0)\n",
    "\n",
    "#     ht_list = ast.literal_eval(row['hashtags'])\n",
    "#     hash_x = np.zeros((1, 1905))\n",
    "#     for ht in ht_list:\n",
    "#       idx = hashtags_set.get(ht, 0)\n",
    "#       if idx!=0:\n",
    "#         hash_x[0, idx-1] = 1\n",
    "\n",
    "#     em_list = ast.literal_eval(row['emojis'])\n",
    "#     emojis_x = np.zeros((1, 162))\n",
    "#     for em in em_list:\n",
    "#       idx = emojis_set.get(em, 0)\n",
    "#       if idx!=0:\n",
    "#         emojis_x[0, idx-1] = 1\n",
    "\n",
    "#     mn_list = ast.literal_eval(row['mentions'])\n",
    "#     mentions_x = np.zeros((1, 1277)) \n",
    "#     for mn in mn_list:\n",
    "#         idx = mentions_set.get(mn, 0)\n",
    "#         if idx!=0:\n",
    "#           mentions_x[0, idx-1] = 1\n",
    "\n",
    "#     x = np.hstack((x, hash_x, emojis_x, mentions_x))\n",
    "    \n",
    "#     train_X = np.vstack((train_X, x))\n",
    "\n",
    "#   return train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqClWiaLze3h"
   },
   "source": [
    "##**Using only 5 class frequencies added up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzYIsaEqwj4C"
   },
   "outputs": [],
   "source": [
    "def generate_train_matrix(train_data):\n",
    "  train_X = np.empty((0, 6))\n",
    "\n",
    "  for index, row in train_data.iterrows():\n",
    "    x = np.zeros((1, 6))\n",
    "    x[0, 0] = 1  # Bias Unit\n",
    "    word_l = indic_tokenize.trivial_tokenize(row['Filtered_Post'], lang='hi')\n",
    "    \n",
    "    for word in word_l:\n",
    "        x[0,1] += freqs.get((word,'defamation'),0)\n",
    "        x[0,2] += freqs.get((word, 'fake'),0)\n",
    "        x[0,3] += freqs.get((word,'hate'),0)\n",
    "        x[0,4] += freqs.get((word,'non-hostile'),0)\n",
    "        x[0,5] += freqs.get((word,'offensive'),0)\n",
    "\n",
    "    ht_list = ast.literal_eval(row['hashtags'])\n",
    "    for ht in ht_list:\n",
    "      x[0,1] += hashtags_set.get((ht,'defamation'),0)\n",
    "      x[0,2] += hashtags_set.get((ht, 'fake'),0)\n",
    "      x[0,3] += hashtags_set.get((ht,'hate'),0)\n",
    "      x[0,4] += hashtags_set.get((ht,'non-hostile'),0)\n",
    "      x[0,5] += hashtags_set.get((ht,'offensive'),0)\n",
    "\n",
    "    em_list = ast.literal_eval(row['emojis'])\n",
    "    for em in em_list:\n",
    "      x[0,1] += emojis_set.get((em,'defamation'),0)\n",
    "      x[0,2] += emojis_set.get((em, 'fake'),0)\n",
    "      x[0,3] += emojis_set.get((em,'hate'),0)\n",
    "      x[0,4] += emojis_set.get((em,'non-hostile'),0)\n",
    "      x[0,5] += emojis_set.get((em,'offensive'),0)\n",
    "\n",
    "    mn_list = ast.literal_eval(row['mentions'])\n",
    "    for mn in mn_list:\n",
    "      x[0,1] += mentions_set.get((mn,'defamation'),0)\n",
    "      x[0,2] += mentions_set.get((mn, 'fake'),0)\n",
    "      x[0,3] += mentions_set.get((mn,'hate'),0)\n",
    "      x[0,4] += mentions_set.get((mn,'non-hostile'),0)\n",
    "      x[0,5] += mentions_set.get((mn,'offensive'),0)\n",
    "  \n",
    "    train_X = np.vstack((train_X, x))\n",
    "\n",
    "  return train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEbwJhTiyo_t"
   },
   "outputs": [],
   "source": [
    "train_X = generate_train_matrix(train_data)\n",
    "val_X = generate_train_matrix(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xq5iTsVQLS3"
   },
   "outputs": [],
   "source": [
    "# Label Mapping\n",
    "labels_mapping = {'defamation':0,\n",
    " 'fake':1,\n",
    " 'hate':2,\n",
    " 'non-hostile':3,\n",
    " 'offensive':4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "eG6v5Su0UcM9",
    "outputId": "62cb3ea4-1833-4564-f3af-0b46e3e208b3"
   },
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhJSn8IpyUsn"
   },
   "outputs": [],
   "source": [
    "train_y = np.empty((0, 5))\n",
    "for index, row in train_data.iterrows():\n",
    "  y = np.zeros((1, 5))\n",
    "  for label in row['Labels Set'].split(','):\n",
    "    y[0, labels_mapping[label]] = 1\n",
    "\n",
    "  train_y = np.vstack((train_y, y))\n",
    "\n",
    "\n",
    "val_y = np.empty((0, 5))\n",
    "for index, row in val_data.iterrows():\n",
    "  y = np.zeros((1, 5))\n",
    "  for label in row['Labels Set'].split(','):\n",
    "    y[0, labels_mapping[label]] = 1\n",
    "\n",
    "  val_y = np.vstack((val_y, y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "bswUa1CJUXH0",
    "outputId": "858fa26b-2779-4fdc-fdae-b654b1016567"
   },
   "outputs": [],
   "source": [
    "print(train_y.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fcQCKsxUok8"
   },
   "source": [
    "##**Binary Relevance Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "CUo-OT5-Un4z",
    "outputId": "10f94815-7a73-48bc-fcfb-0d40c0e34bed"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skmultilearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8f24d1ca13ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryRelevance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skmultilearn'"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = BinaryRelevance(LogisticRegression(max_iter=150))\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J-3b1S_U8Ld"
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atQIjBLfVoz5"
   },
   "outputs": [],
   "source": [
    "def evaluation(y_true, y_pred):\n",
    "  print(\"Fine Grained Accuracy = {}\".format(accuracy_score(y_true, y_pred)))\n",
    "  print(\"\\n\\nFine Grained Metrics\\n\")\n",
    "  print(classification_report(y_true, y_pred))\n",
    "\n",
    "  # y_true_coarse = np.zeros((y_true.shape[0], 2))\n",
    "  # y_pred_coarse = np.zeros((y_true.shape[0], 2))\n",
    "\n",
    "  # y_true_coarse_1 = y_true[:,3] \n",
    "  # y_true_coarse_0 = 1 - y_true_coarse[:,1]\n",
    "  # y_true_coarse = np.hstack((y_true_coarse_0, y_true_coarse_1))\n",
    "  \n",
    "  # y_pred_coarse_1 = y_pred[:,3]\n",
    "  # y_pred_coarse_0 = 1- y_pred_coarse[:,1]\n",
    "  # y_pred_coarse = np.hstack((y_pred_coarse_0, y_pred_coarse_1))\n",
    "\n",
    "  # print(\"Coarse Grained Accuracy = {}\".format(accuracy_score(y_true_coarse, y_pred_coarse)))\n",
    "  # print(\"\\n\\nCoarse Grained Metrics\\n\")\n",
    "  # print(classification_report(y_true_coarse, y_pred_coarse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "1my9tQz-6xCB",
    "outputId": "a1ae4716-aee5-435f-918b-f9f4a01a08ae"
   },
   "outputs": [],
   "source": [
    "evaluation(val_y, predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BinaryRelevance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
